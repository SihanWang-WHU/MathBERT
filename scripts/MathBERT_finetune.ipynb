{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MathBERT_finetune.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3Bex7ap4oSHi"},"source":["#### Open a TPU session and mount your drive to Google Colab"]},{"cell_type":"code","metadata":{"id":"vPL7bLSXn3V5"},"source":["%tensorflow_version 1.x\n","import os\n","import pprint\n","import json\n","import tensorflow as tf\n","\n","assert \"COLAB_TPU_ADDR\" in os.environ, \"ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\"\n","TPU_ADDRESS = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"] \n","TPU_TOPOLOGY = \"2x2\"\n","print(\"TPU address is\", TPU_ADDRESS)\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","with tf.Session(TPU_ADDRESS) as session:\n","  print('TPU devices:')\n","  pprint.pprint(session.list_devices())\n","\n","  # Upload credentials to TPU. \n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","    # Now credentials are set for all future sessions on this TPU."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWQef8cBoDaD"},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/your_working_dir')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mV2HPxjZoMb4"},"source":["#### Download tensorflow checkpoint from S3 location provided\n","+ MathBERT-basevocab-uncased model artifacts are stored in s3 bucket and can be downloaded using 'wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/bert_config.json'\n","+ MathBERT-mathvocab-uncased model artifacts are stored in s3 bucket and can be downloaded using 'wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-mathvocab-uncased/bert_config.json'"]},{"cell_type":"code","metadata":{"id":"HSl8tzzVwYwC"},"source":["!mkdir your_model_folder\n","%cd your_model_folder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUlu0e0toD1K","outputId":"cb2f9595-c355-4e89-b5b2-c34479661974"},"source":["!wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/bert_config.json\n","!wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/vocab.txt\n","!wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/bert_model.ckpt.index\n","!wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/bert_model.ckpt.meta\n","!wget http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/bert_model.ckpt.data-00000-of-00001"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-20 02:47:06--  http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/bert_config.json\n","Resolving tracy-nlp-models.s3.amazonaws.com (tracy-nlp-models.s3.amazonaws.com)... 52.216.101.115\n","Connecting to tracy-nlp-models.s3.amazonaws.com (tracy-nlp-models.s3.amazonaws.com)|52.216.101.115|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 570 [application/json]\n","Saving to: ‘bert_config.json’\n","\n","\rbert_config.json      0%[                    ]       0  --.-KB/s               \rbert_config.json    100%[===================>]     570  --.-KB/s    in 0s      \n","\n","2021-05-20 02:47:06 (78.6 MB/s) - ‘bert_config.json’ saved [570/570]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PDSvYILZvl_j"},"source":["#### Download MathBERT training scripts\n","+ git clone the MathBERT repo at https://github.com/tbs17/MathBERT/\n"]},{"cell_type":"code","metadata":{"id":"1VJThb1-voo8"},"source":["%cd ../ #clone the repo outside of your model folder\n","!git clone https://github.com/tbs17/MathBERT.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etceOFRUvosB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtwgtVfWvout"},"source":["#### Start Fine-tuning\n","+ prepare your dataset to be compatible with BERT training data format. Please refer to the details at https://github.com/google-research/bert. \n","+ Alternatively, you can use our function 'split_3data_label()' to split your data into 3 parts and save your label data\n","\n","+ using the below script to utilize the MathBERT artifact for fine-tuning\n","+ for the 3 downstream tasks we had, we used the below task name:\n","    + skill code prediction: 'COLA_385'\n","    + auto-grading classification: 'auto_grade'\n","    + knowledge tracing classificationsingle: 'KT'\n"," + depending on your number of labels, just change the line accordingly in the run_classifier.py\n","    ```\n","        def get_labels(self):\n","            return [str(x) for x in range(your_num_labels)]\n","    ```"]},{"cell_type":"code","metadata":{"id":"k8U2CyHY1nVb"},"source":["def split_3data_label(org_path,out_dir):\n","    import pandas as pd\n","    import os\n","    from sklearn.model_selection import train_test_split\n","    data=pd.read_csv(org_path,encoding='utf-8',names=['text','label'],header=0)\n","    \n","    print(f'total sample is {data.shape[0]}')\n","    df_train, df_test=train_test_split(data,test_size=0.2,random_state=111)\n","\n","    df_bert_train, df_bert_dev = train_test_split(df_train, test_size=0.1,random_state=111)\n","    #create new dataframe for test data\n","\n","    #output tsv file, no header for train and dev\n","    if not os.path.exists(out_dir):\n","        os.makedirs(out_dir)\n","    df_bert_train.to_csv('{}/train_with_label.csv'.format(out_dir), index=False)\n","    df_bert_dev.to_csv('{}/dev_with_label.csv'.format(out_dir),index=False)\n","    df_test.to_csv('{}/test_with_label.csv'.format(out_dir), index=False)\n","    print(f'training samples are {df_bert_train.shape[0]}\\n'\n","        f'eval samples are {df_bert_dev.shape[0]}\\n'\n","        f'testing samples are {df_test.shape[0]}'\n","        )\n","#     print('{} unique labels'.format(data0['label_en'].nunique()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08PPLWv6wrRG"},"source":["# Please find the full list of tasks and their fintuning hyperparameters\n","# here https://github.com/google-research/albert/blob/master/run_glue.sh\n","\n","BUCKET = \"your_google_cloud_bucket\" #@param { type: \"string\" }\n","TASK = 'auto_grade' #@param {type:\"string\"}\n","\n","BASE_DIR = \"gs://\" + BUCKET\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BUCKET.\")\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","\n","OUTPUT_DIR = 'gs://{}/BERT/mathBERT/{}_{}'.format(BUCKET, TASK,'MathBERT_LR2E-5_BS64_MS512_EP5_baseVocab_testing')\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVfrHOGKynBk"},"source":["import time\n","start=time.time()\n","os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n","!python bert/run_classifier.py \\\n","  --data_dir=mathBERT-downstreamTasks/auto_grade/ \\\n","  --bert_config_file=Upload_Models/MathBERT-orig/bert_config.json \\\n","  --vocab_file=Upload_Models/MathBERT-orig/vocab.txt \\\n","  --task_name=$TASK \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --init_checkpoint='gs://your_bucket/MathBERT-basevocab-uncased/bert_model.ckpt' \\\n","  --do_lower_case=True \\\n","  --do_train=True \\\n","  --do_eval=True \\\n","  --do_predict=True \\\n","  --max_seq_length=512 \\\n","  --warmup_step=1000 \\\n","  --learning_rate=2e-5 \\\n","  --num_train_epochs=5 \\\n","  --save_checkpoints_steps=2000 \\\n","  --train_batch_size=64 \\\n","  --eval_batch_size=32 \\\n","  --predict_batch_size=16 \\\n","  --tpu_name=$TPU_ADDRESS \\\n","  --use_tpu=True\n","end=time.time()\n","print(f'it took {(end-start)/60} mins to finish ')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2LGHeoJy1KW"},"source":["+ Downloading the prediction result and eval result to your local folder"]},{"cell_type":"code","metadata":{"id":"Dv-SwhSny68e"},"source":["import os\n","folder_name=OUTPUT_DIR.split('/')[5]\n","os.environ['OUTPUT_DIR']=OUTPUT_DIR\n","os.environ['folder_name']=folder_name\n","!gsutil cp $OUTPUT_DIR/eval_results.txt your_output_folder/$folder_name-eval_results.txt\n","!gsutil cp $OUTPUT_DIR/test_results.tsv your_output_folder/$folder_name-test_results.tsv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-thmx0KzIOF"},"source":["#### Convert Testing result \n","+ the prediction output need to be formated so that you can evaluate \n","+ below examples are for predicting the auto-grading and KC tasks with 5 labels and 385 labels in the dataset"]},{"cell_type":"code","metadata":{"id":"Mz9_ZwdkzIad"},"source":["def match_top3_f1_acc(data_dir,label_Path,out_dir):\n","    from datetime import datetime\n","    import numpy as np\n","    import pandas as pd\n","    import os\n","    from sklearn.metrics import accuracy_score\n","    from sklearn.metrics import f1_score\n","    label_data=pd.read_csv(label_Path,names=['label'],header=0)\n","    print(f'test data shape{label_data.shape}')\n","    for i, file in enumerate(os.listdir(data_dir)):\n","    \n","        pred_dataPath=os.path.join(data_dir,file)\n","        print('======')\n","        print(i,pred_dataPath)\n","        \n","        if pred_dataPath.endswith('.csv'):\n","\n","            pred_data=pd.read_csv(pred_dataPath,encoding='ISO-8859-1')\n","            print(f'predicted data shape: {pred_data.shape}')\n","            print('total {} classes are predicted as top class'.format(len(pred_data['top1'].unique())))\n","            match=pd.concat([pred_data,label_data],axis=1)\n","            print(f'merged data shape{match.shape}')\n","            correct_top1=match[match['top1']==match['label']].top1.unique()\n","            correct_top2=match[match['top2']==match['label']].top2.unique()\n","            correct_top3=match[match['top3']==match['label']].top3.unique()\n","            correct_all=list(set(list(correct_top1)+list(correct_top2)+list(correct_top3)))\n","            print('Correct top 1 label {}'.format(len(correct_top1)))\n","            print('Correct top 2 label {}'.format(len(correct_top2)))\n","            print('Correct top 3 label {}'.format(len(correct_top3)))\n","            print('Total top 3 Correct labels {}'.format(len(correct_all)))\n","            match['matched1']=np.where(match['top1']==match['label'],1,0)\n","            match['matched2']=np.where((match['top1']==match['label']) | (match['top2']==match['label']),1,0)\n","            match['matched3']=np.where((match['top1']==match['label']) | (match['top2']==match['label']) | (match['top3']==match['label']),1,0)\n","       \n","            marker=file.split('.')[0]\n","            from pathlib import Path\n","            out_dir=Path(out_dir)\n","            out_dir.mkdir(exist_ok=True)\n","            match.to_csv('{}/{}_matched.csv'.format(out_dir,marker),index=False)\n","\n","            \n","            print('---Below is F1 Score(weighted)--')\n","            top1_f1=round(f1_score(match['label'], match['top1'], average='weighted')*100,3)\n","            print('Top1 label F1 score(weighted): {}%'.format(top1_f1))\n","            top2_f1=round(f1_score(match['label'], match['top2'], average='weighted')*100,3)\n","            print('Top1 label F1 score(weighted): {}%'.format(top2_f1+top1_f1))\n","            top3_f1=round(f1_score(match['label'], match['top3'], average='weighted')*100,3)\n","            print('Top1 label F1 score(weighted): {}%'.format(top3_f1+top2_f1+top1_f1))\n","            \n","            print ('---Below is Sklearn accuracy---')\n","            top1_accuracy=round(accuracy_score(match['label'], match['top1'])*100,3)\n","            print('Top1 label accuracy score: {}%'.format(top1_accuracy))\n","            top2_accuracy=round(accuracy_score(match['label'], match['top2'])*100,3)\n","            print('Top1 label accuracy score: {}%'.format(top2_accuracy+top1_accuracy))\n","            top3_accuracy=round(accuracy_score(match['label'], match['top3'])*100,3)\n","            print('Top1 label accuracy score: {}%'.format(top3_accuracy+top2_accuracy+top1_accuracy))\n","\n","\n","def convert_autoGrade(orig_testPath,pred_testPath,org_dataPath,out_dir):\n","    from pathlib import Path\n","    import pandas as pd\n","    #read the original test data for the text and id\n","    df_test = pd.read_csv(orig_testPath, sep='\\t')#,engine='python'\n","    df_test['guid']=df_test.iloc[:,0].astype(str)\n","    print(f'original test file has shape {df_test.shape}')\n","    #read the results data for the probabilities\n","    df_result = pd.read_csv(pred_testPath, sep='\\t', header=None)\n","    print(f'predicted test file has shape {df_result.shape}')\n","    out_dir=Path(out_dir)\n","    Path.mkdir(out_dir,exist_ok=True)\n","    import numpy as np\n","    # df_map\n","    df_map_result = pd.DataFrame({'guid': df_test['guid'],\n","        'question': df_test['question'],\n","        'answer': df_test['answer'],\n","        'top1': df_result.idxmax(axis=1),\n","        'top1_probability':df_result.max(axis=1),\n","        'top2': df_result.columns[df_result.values.argsort(1)[:,-2]],\n","        'top2_probability':df_result.apply(lambda x: sorted(x)[-2],axis=1),\n","        'top3': df_result.columns[df_result.values.argsort(1)[:,-3]],\n","        'top3_probability':df_result.apply(lambda x: sorted(x)[-3],axis=1),\n","        'top4': df_result.columns[df_result.values.argsort(1)[:,-4]],\n","        'top4_probability':df_result.apply(lambda x: sorted(x)[-4],axis=1),\n","        'top5': df_result.columns[df_result.values.argsort(1)[:,-5]],\n","        'top5_probability':df_result.apply(lambda x: sorted(x)[-5],axis=1)\n","        })\n","    #view sample rows of the newly created dataframe\n","#     display(df_map_result.head())\n","    df_map_result['top1']=df_map_result['top1'].astype(str)\n","    df_map_result['top2']=df_map_result['top2'].astype(str)\n","    df_map_result['top3']=df_map_result['top3'].astype(str)\n","    df_map_result.dtypes\n","    df_map_result['top4']=df_map_result['top4'].astype(str)\n","    df_map_result['top5']=df_map_result['top5'].astype(str)\n","    print(f'mapped test file has shape {df_map_result.shape}')\n","\n","    label_map_dict={'0':1,'1':2,'2':3,'3':4,'4':5}\n","    marker=pred_testPath.split('/')[-1].split('.')[0]\n","    df_map_result=df_map_result.replace({'top1':label_map_dict,'top2':label_map_dict,'top3':label_map_dict,'top4':label_map_dict,'top5':label_map_dict})\n","    df_map_result.to_csv('{}/{}_converted.csv'.format(out_dir,marker),index=False)\n","    print(df_map_result.shape)#(702, 12)\n","    return df_map_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39lH4BEa0ghz"},"source":["orig_testPath='auto_grade/test.tsv'#original test data\n","pred_testPath='output/auto_grade_MathBERT_LR2E-5_BS64_MS512_EP5_baseVocab_testing-test_results.tsv' #your own predicted test data\n","org_dataPath='auto_grade/auto_grade_original_full_data.csv'#the original full set of auto grade data\n","out_dir='TEST_converted_autoGrade_MATHBERT'# name your own output dir\n","df_map_result=convert_autoGrade(orig_testPath,pred_testPath,org_dataPath,out_dir)\n","df_map_result.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzQ0wEJjfEJX"},"source":["orig_testPath='TAPT/skill_code_DESC_TITLE_PROBLEM/CoLA/test.tsv'#original test data\n","pred_testPath='EVAL_RESULT/MathBERT/COLA_385_MathBERT-TAPT_LR5E-5_BS64_MS512_EP25_customVocab-V2_FIT_SEED5_V2-test_results.tsv'#your own predicted test data\n","org_dataPath='further-pre-training/CORPUS/ALL_GRADES/DESC_TITLE_PROBLEM_combined.csv'#the original full set \n","out_dir='TEST_converted_DESC_TITLE_PROB_MATHBERT'# name your own output dir\n","df_map_result=convert_test_result(orig_testPath,pred_testPath,org_dataPath,out_dir)\n","df_map_result.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCKX4RXSzrQb"},"source":["#### Evaluate for F1, Accuracy and AUC"]},{"cell_type":"code","metadata":{"id":"v9fVPmScznAb"},"source":["def match_top3_f1_acc_autoGrade(data_dir,label_Path,out_dir):\n","    from datetime import datetime\n","    import numpy as np\n","    import pandas as pd\n","    import os\n","    from sklearn.metrics import roc_curve, accuracy_score\n","    from sklearn.metrics import f1_score, roc_auc_score\n","    label_data=pd.read_csv(label_Path,names=['Index','label'],header=0)\n","    label_data_nna=label_data.dropna()\n","    \n","    print(f'test data shape{label_data.shape}')\n","    print(f'test data shape after dropping na {label_data_nna.shape}')\n","    for i, file in enumerate(os.listdir(data_dir)):\n","    \n","        pred_dataPath=os.path.join(data_dir,file)\n","        print('======')\n","        print(i, pred_dataPath)\n","\n","        if pred_dataPath.endswith('.csv'):\n","        \n","            pred_data=pd.read_csv(pred_dataPath,encoding='ISO-8859-1')\n","            print(f'predicted data shape: {pred_data.shape}')\n","            print('total {} classes are predicted as top class'.format(len(pred_data['top1'].unique())))\n","            match=pd.concat([pred_data,label_data_nna],axis=1)\n","            print(f'merged data shape{match.shape}')\n","            correct_top1=match[match['top1']==match['label']].top1.unique()\n","            correct_top2=match[match['top2']==match['label']].top2.unique()\n","            correct_top3=match[match['top3']==match['label']].top3.unique()\n","            correct_all=list(set(list(correct_top1)+list(correct_top2)+list(correct_top3)))\n","            print('Correct top 1 label {}'.format(len(correct_top1)))\n","            print('Correct top 2 label {}'.format(len(correct_top2)))\n","            print('Correct top 3 label {}'.format(len(correct_top3)))\n","            print('Total top 3 Correct labels {}'.format(len(correct_all)))\n","            match['matched1']=np.where(match['top1']==match['label'],1,0)\n","            match['matched2']=np.where((match['top1']==match['label']) | (match['top2']==match['label']),1,0)\n","            match['matched3']=np.where((match['top1']==match['label']) | (match['top2']==match['label']) | (match['top3']==match['label']),1,0)\n","       \n","            marker=file.split('.')[0]\n","            from pathlib import Path\n","            out_dir=Path(out_dir)\n","            out_dir.mkdir(exist_ok=True)\n","            match.to_csv('{}/{}_matched.csv'.format(out_dir,marker),index=False)\n","            # display(match.head())\n","            \n","            print('---Below is F1 Score(weighted)--')\n","            top1_f1=round(f1_score(match['label'], match['top1'], average='weighted')*100,3)\n","            print('Top1 label F1 score(weighted): {}%'.format(top1_f1))\n","            top2_f1=round(f1_score(match['label'], match['top2'], average='weighted')*100,3)\n","            print('Top1 label F1 score(weighted): {}%'.format(top2_f1+top1_f1))\n","            top3_f1=round(f1_score(match['label'], match['top3'], average='weighted')*100,3)\n","            print('Top1 label F1 score(weighted): {}%'.format(top3_f1+top2_f1+top1_f1))\n","            print ('---Below is Sklearn accuracy---')\n","            top1_accuracy=round(accuracy_score(match['label'], match['top1'])*100,3)\n","            print('Top1 label accuracy score: {}%'.format(top1_accuracy))\n","            top2_accuracy=round(accuracy_score(match['label'], match['top2'])*100,3)\n","            print('Top1 label accuracy score: {}%'.format(top2_accuracy+top1_accuracy))\n","            top3_accuracy=round(accuracy_score(match['label'], match['top3'])*100,3)\n","            print('Top1 label accuracy score: {}%'.format(top3_accuracy+top2_accuracy+top1_accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJkloBmvzn3O"},"source":["def convert_auc_autoGrade(orig_testPath,pred_testDir,label_Path,org_dataPath,out_dir):\n","    from pathlib import Path\n","    import pandas as pd\n","    from sklearn.metrics import roc_auc_score\n","    import numpy as np\n","    import os\n","    #read the original test data for the text and id\n","    df_test = pd.read_csv(orig_testPath, sep='\\t',engine='python')\n","    df_test['guid']=df_test.iloc[:,0].astype(str)\n","    print(f'original test file has shape {df_test.shape}')\n","    print('----')\n","    out_dir=Path(out_dir)\n","    Path.mkdir(out_dir,exist_ok=True)\n","    #read the results data for the probabilities\n","    for i, f in enumerate(os.listdir(pred_testDir)):\n","        if f.endswith('test_results.tsv'):\n","            print(i, f)\n","        \n","            df_result = pd.read_csv(os.path.join(pred_testDir,f), sep='\\t', header=None)\n","            print(f'predicted test file has shape {df_result.shape}')\n","            label_data=pd.read_csv(label_Path,usecols=['label'])\n","            auc=round(roc_auc_score(label_data['label'], df_result,multi_class='ovo',average='weighted')*100,3)\n","            print(f'average auc for 5 classes is {auc} %!')\n","            print('----')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ziMy7hd0KEU"},"source":["data_dir='TEST_converted_autoGrade_MATHBERT'\n","label_Path='auto_grade/test_labels.csv'\n","out_dir='AUTO_GRADE_matched_MATHBERT'\n","match_top3_f1_acc_autoGrade(data_dir,label_Path,out_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vCb-0-9C0-Yc"},"source":["orig_testPath='auto_grade/test.tsv'\n","pred_testDir='MathBERT/AUTO-GRADE/'\n","org_dataPath='auto_grade/auto_grade_original_full_data.csv'\n","label_Path='auto_grade/test_labels.csv'\n","out_dir='TEST_converted_autoGrade_MATHBERT'\n","convert_auc_autoGrade(orig_testPath,pred_testDir,label_Path,org_dataPath,out_dir)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fjMdkE3e2ge"},"source":["orig_testPath='TAPT/skill_code_DESC_TITLE_PROBLEM/CoLA/test.tsv'\n","pred_testDir='EVAL_RESULT/MathBERT/skillCode_TAPT/'\n","org_dataPath='further-pre-training/CORPUS/ALL_GRADES/DESC_TITLE_PROBLEM_combined.csv'\n","label_Path='TAPT/skill_code_DESC_TITLE_PROBLEM/CoLA/test_labels.csv'\n","out_dir='TEST_converted_DESC_TITLE_PROB_MATHTAPT_V2'\n","convert_auc_skillCode(orig_testPath,pred_testDir,label_Path,org_dataPath,out_dir)\n","# df_map_result.head()"],"execution_count":null,"outputs":[]}]}